{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidharth-j/SCIT/blob/master/Deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwcTPCVlLasA",
        "colab_type": "code",
        "outputId": "2c5536b8-ecf5-4916-c80a-a0b92a1f8586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#Import the libraries required\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "import time\n",
        "\n",
        "\n",
        "#Splitting the data to test and train\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "#Convert the images to vectors by flattening them\n",
        "image_vector_size = 28*28\n",
        "x_train = x_train.reshape(x_train.shape[0], image_vector_size)\n",
        "x_test = x_test.reshape(x_test.shape[0], image_vector_size)\n",
        "\n",
        "\n",
        "\n",
        "#Convert the output to one-hot encoded vectors\n",
        "num_classes = 10                                          \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "#Creating a sequential model \n",
        "model = Sequential()\n",
        "#Adding the first hidden layer with 32 nodes. The input layer has 784 nodes\n",
        "model.add(Dense(units=397, activation='sigmoid', kernel_initializer='uniform', input_shape=(image_vector_size,)))\n",
        "#Adding the output layer\n",
        "model.add(Dense(units=num_classes, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "#Training the model\n",
        "\n",
        "model.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=15, verbose=False, validation_split=.1)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "loss, accuracy  = model.evaluate(x_test, y_test, verbose=False)\n",
        "print(accuracy)\n",
        "\n",
        "time_taken = end - start                                                                #time taken\n",
        "print('Time taken: ',time_taken)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n",
            "(10000, 10)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 397)               311645    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                3980      \n",
            "=================================================================\n",
            "Total params: 315,625\n",
            "Trainable params: 315,625\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "0.9567\n",
            "Time taken:  47.58606481552124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHe0EzZQdkop",
        "colab_type": "code",
        "outputId": "52a28321-c323-424f-8182-4567f000cded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn import linear_model, datasets, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import clone\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Turn down for faster convergence\n",
        "train_samples = 60000\n",
        "\n",
        "# Load data from https://www.openml.org/d/554\n",
        "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, train_size=train_samples, test_size=10000)\n",
        "\n",
        "##############################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_train = x_train.astype(\"float32\")\n",
        "x_test = x_test.astype(\"float32\")\n",
        "\n",
        "x_train = x_train/255 # 0-1 scaling\n",
        "x_test =  x_test/255\n",
        "y_train = y_train\n",
        "y_test = y_test\n",
        "\n",
        "\n",
        "logistic = linear_model.LogisticRegression(solver='newton-cg', tol=1,\n",
        "                                           multi_class='multinomial')\n",
        "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
        "\n",
        "rbm_features_classifier = Pipeline(\n",
        "    steps=[('rbm', rbm), ('logistic', logistic)])\n",
        "\n",
        "\n",
        "\n",
        "# #############################################################################\n",
        "# Training\n",
        "\n",
        "rbm.learning_rate = 0.005\n",
        "rbm.n_iter = 7\n",
        "# More components tend to give better prediction performance, but larger\n",
        "# fitting time\n",
        "rbm.n_components = 100\n",
        "logistic.C = 10.0\n",
        "\n",
        "# Training RBM-Logistic Pipeline\n",
        "\n",
        "\n",
        "rbm_features_classifier.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # #############################################################################\n",
        "# # Evaluation\n",
        "\n",
        "y_pred = rbm_features_classifier.predict(x_test) \n",
        "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
        "    metrics.accuracy_score(y_test, y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -125.69, time = 14.63s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -108.77, time = 16.38s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -100.25, time = 16.65s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -95.94, time = 16.44s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -93.44, time = 16.38s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -89.89, time = 16.33s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -87.76, time = 16.85s\n",
            "Logistic regression using RBM features:\n",
            "0.935\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB_ESYcTZi0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+git://github.com/albertbup/deep-belief-network.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPJPVqJFPhP0",
        "colab_type": "code",
        "outputId": "45193cee-0f3f-4b04-bc50-23d14b11861e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2842
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.classification import accuracy_score\n",
        "\n",
        "from dbn.tensorflow import SupervisedDBNClassification\n",
        "# use \"from dbn import SupervisedDBNClassification\" for computations on CPU with numpy\n",
        "\n",
        "# Loading dataset\n",
        "digits = load_digits()\n",
        "X, Y = digits.data, digits.target\n",
        "\n",
        "# Data scaling\n",
        "X = (X / 16).astype(np.float32)\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Training\n",
        "classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
        "                                         learning_rate_rbm=0.05,\n",
        "                                         learning_rate=0.1,\n",
        "                                         n_epochs_rbm=10,\n",
        "                                         n_iter_backprop=100,\n",
        "                                         batch_size=32,\n",
        "                                         activation_function='relu',\n",
        "                                         dropout_p=0.2)\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Save the model\n",
        "classifier.save('model.pkl')\n",
        "\n",
        "# Restore it\n",
        "classifier = SupervisedDBNClassification.load('model.pkl')\n",
        "\n",
        "# Test\n",
        "Y_pred = classifier.predict(X_test)\n",
        "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/albertbup/deep-belief-network.git\n",
            "  Cloning git://github.com/albertbup/deep-belief-network.git to /tmp/pip-req-build-jqntxpx2\n",
            "  Running command git clone -q git://github.com/albertbup/deep-belief-network.git /tmp/pip-req-build-jqntxpx2\n",
            "Requirement already satisfied (use --upgrade to upgrade): deep-belief-network==1.0.3 from git+git://github.com/albertbup/deep-belief-network.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from deep-belief-network==1.0.3) (1.16.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from deep-belief-network==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from deep-belief-network==1.0.3) (0.21.1)\n",
            "Requirement already satisfied: tensorflow>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from deep-belief-network==1.0.3) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.1->deep-belief-network==1.0.3) (0.12.5)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.13.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.0.9)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (0.33.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.13.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (3.7.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow>=1.5.0->deep-belief-network==1.0.3) (3.0.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.5.0->deep-belief-network==1.0.3) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.5.0->deep-belief-network==1.0.3) (0.15.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow>=1.5.0->deep-belief-network==1.0.3) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow>=1.5.0->deep-belief-network==1.0.3) (41.0.1)\n",
            "Building wheels for collected packages: deep-belief-network\n",
            "  Building wheel for deep-belief-network (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wlhbgpg9/wheels/29/6d/3b/6a50cf42a32bdfaa903b17832d60d8d3e5dc4b0fd02ae8acaf\n",
            "Successfully built deep-belief-network\n",
            "[START] Pre-training step:\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/dbn/tensorflow/models.py:150: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 3.254259\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 2.158452\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 1.823412\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 1.556917\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 1.409843\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 1.270320\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 1.267828\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 1.141831\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 1.050468\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 0.989886\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 2.821056\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 2.101578\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 1.274505\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 1.145565\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 0.913202\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 0.753634\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 0.759398\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 0.764588\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 0.925962\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 0.665027\n",
            "[END] Pre-training step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/dbn/tensorflow/models.py:338: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "[START] Fine tuning step:\n",
            ">> Epoch 0 finished \tANN training loss 1.352018\n",
            ">> Epoch 1 finished \tANN training loss 0.725335\n",
            ">> Epoch 2 finished \tANN training loss 0.557961\n",
            ">> Epoch 3 finished \tANN training loss 0.418931\n",
            ">> Epoch 4 finished \tANN training loss 0.359973\n",
            ">> Epoch 5 finished \tANN training loss 0.307108\n",
            ">> Epoch 6 finished \tANN training loss 0.283878\n",
            ">> Epoch 7 finished \tANN training loss 0.251663\n",
            ">> Epoch 8 finished \tANN training loss 0.224139\n",
            ">> Epoch 9 finished \tANN training loss 0.219445\n",
            ">> Epoch 10 finished \tANN training loss 0.190695\n",
            ">> Epoch 11 finished \tANN training loss 0.184894\n",
            ">> Epoch 12 finished \tANN training loss 0.168811\n",
            ">> Epoch 13 finished \tANN training loss 0.167626\n",
            ">> Epoch 14 finished \tANN training loss 0.150347\n",
            ">> Epoch 15 finished \tANN training loss 0.156646\n",
            ">> Epoch 16 finished \tANN training loss 0.167085\n",
            ">> Epoch 17 finished \tANN training loss 0.134383\n",
            ">> Epoch 18 finished \tANN training loss 0.134961\n",
            ">> Epoch 19 finished \tANN training loss 0.124756\n",
            ">> Epoch 20 finished \tANN training loss 0.128623\n",
            ">> Epoch 21 finished \tANN training loss 0.119035\n",
            ">> Epoch 22 finished \tANN training loss 0.110684\n",
            ">> Epoch 23 finished \tANN training loss 0.122329\n",
            ">> Epoch 24 finished \tANN training loss 0.121494\n",
            ">> Epoch 25 finished \tANN training loss 0.101220\n",
            ">> Epoch 26 finished \tANN training loss 0.098956\n",
            ">> Epoch 27 finished \tANN training loss 0.103067\n",
            ">> Epoch 28 finished \tANN training loss 0.100085\n",
            ">> Epoch 29 finished \tANN training loss 0.097945\n",
            ">> Epoch 30 finished \tANN training loss 0.115213\n",
            ">> Epoch 31 finished \tANN training loss 0.089979\n",
            ">> Epoch 32 finished \tANN training loss 0.090189\n",
            ">> Epoch 33 finished \tANN training loss 0.084322\n",
            ">> Epoch 34 finished \tANN training loss 0.092524\n",
            ">> Epoch 35 finished \tANN training loss 0.080723\n",
            ">> Epoch 36 finished \tANN training loss 0.086343\n",
            ">> Epoch 37 finished \tANN training loss 0.079419\n",
            ">> Epoch 38 finished \tANN training loss 0.080496\n",
            ">> Epoch 39 finished \tANN training loss 0.076376\n",
            ">> Epoch 40 finished \tANN training loss 0.078789\n",
            ">> Epoch 41 finished \tANN training loss 0.071091\n",
            ">> Epoch 42 finished \tANN training loss 0.071127\n",
            ">> Epoch 43 finished \tANN training loss 0.063924\n",
            ">> Epoch 44 finished \tANN training loss 0.062440\n",
            ">> Epoch 45 finished \tANN training loss 0.072118\n",
            ">> Epoch 46 finished \tANN training loss 0.075461\n",
            ">> Epoch 47 finished \tANN training loss 0.066835\n",
            ">> Epoch 48 finished \tANN training loss 0.070710\n",
            ">> Epoch 49 finished \tANN training loss 0.061123\n",
            ">> Epoch 50 finished \tANN training loss 0.071400\n",
            ">> Epoch 51 finished \tANN training loss 0.053185\n",
            ">> Epoch 52 finished \tANN training loss 0.060053\n",
            ">> Epoch 53 finished \tANN training loss 0.070100\n",
            ">> Epoch 54 finished \tANN training loss 0.056444\n",
            ">> Epoch 55 finished \tANN training loss 0.059640\n",
            ">> Epoch 56 finished \tANN training loss 0.057362\n",
            ">> Epoch 57 finished \tANN training loss 0.052480\n",
            ">> Epoch 58 finished \tANN training loss 0.053354\n",
            ">> Epoch 59 finished \tANN training loss 0.063251\n",
            ">> Epoch 60 finished \tANN training loss 0.055590\n",
            ">> Epoch 61 finished \tANN training loss 0.053554\n",
            ">> Epoch 62 finished \tANN training loss 0.063067\n",
            ">> Epoch 63 finished \tANN training loss 0.049388\n",
            ">> Epoch 64 finished \tANN training loss 0.050863\n",
            ">> Epoch 65 finished \tANN training loss 0.049146\n",
            ">> Epoch 66 finished \tANN training loss 0.048804\n",
            ">> Epoch 67 finished \tANN training loss 0.064511\n",
            ">> Epoch 68 finished \tANN training loss 0.056312\n",
            ">> Epoch 69 finished \tANN training loss 0.046292\n",
            ">> Epoch 70 finished \tANN training loss 0.049946\n",
            ">> Epoch 71 finished \tANN training loss 0.046645\n",
            ">> Epoch 72 finished \tANN training loss 0.046396\n",
            ">> Epoch 73 finished \tANN training loss 0.047512\n",
            ">> Epoch 74 finished \tANN training loss 0.046153\n",
            ">> Epoch 75 finished \tANN training loss 0.043379\n",
            ">> Epoch 76 finished \tANN training loss 0.041525\n",
            ">> Epoch 77 finished \tANN training loss 0.044932\n",
            ">> Epoch 78 finished \tANN training loss 0.055020\n",
            ">> Epoch 79 finished \tANN training loss 0.041940\n",
            ">> Epoch 80 finished \tANN training loss 0.040156\n",
            ">> Epoch 81 finished \tANN training loss 0.033572\n",
            ">> Epoch 82 finished \tANN training loss 0.039327\n",
            ">> Epoch 83 finished \tANN training loss 0.039610\n",
            ">> Epoch 84 finished \tANN training loss 0.039920\n",
            ">> Epoch 85 finished \tANN training loss 0.042059\n",
            ">> Epoch 86 finished \tANN training loss 0.035120\n",
            ">> Epoch 87 finished \tANN training loss 0.036255\n",
            ">> Epoch 88 finished \tANN training loss 0.032378\n",
            ">> Epoch 89 finished \tANN training loss 0.035266\n",
            ">> Epoch 90 finished \tANN training loss 0.032732\n",
            ">> Epoch 91 finished \tANN training loss 0.035787\n",
            ">> Epoch 92 finished \tANN training loss 0.043460\n",
            ">> Epoch 93 finished \tANN training loss 0.044086\n",
            ">> Epoch 94 finished \tANN training loss 0.036203\n",
            ">> Epoch 95 finished \tANN training loss 0.041078\n",
            ">> Epoch 96 finished \tANN training loss 0.031966\n",
            ">> Epoch 97 finished \tANN training loss 0.034226\n",
            ">> Epoch 98 finished \tANN training loss 0.030029\n",
            ">> Epoch 99 finished \tANN training loss 0.044455\n",
            "[END] Fine tuning step\n",
            "Done.\n",
            "Accuracy: 0.994444\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}